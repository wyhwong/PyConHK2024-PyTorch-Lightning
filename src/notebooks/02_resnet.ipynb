{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook: Exploration on More Usage of PyTorch Lightning\n",
    "\n",
    "This notebook is a continuation of the previous notebook on PyTorch Lightning. In this notebook, we will explore more on the usage of PyTorch Lightning. We will cover the following topics:\n",
    "- Training Monitoring (Visualization using TensorBoard)\n",
    "- Callbacks (Lightning built-in callbacks and customized callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variables for the Slack API\n",
    "os.environ[\"SLACK_APP_TOKEN\"] = \"\"\n",
    "os.environ[\"SLACK_BOT_TOKEN\"] = \"\"\n",
    "os.environ[\"SLACK_DEFAULT_CHANNEL_ID\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import demo.lightning.models\n",
    "import demo.lightning.callbacks.slack\n",
    "import demo.slack.client\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset: Initialize the CIFAR10 dataset for image classification task\n",
    "\n",
    "In this section, we initialize the CIFAR10 dataset and dataloaders. The dataset is prepared for image classification task. We will use this dataset to demonstrate the usage of PyTorch Lightning. \n",
    "\n",
    "#### Notes for CIFAR10 dataset:\n",
    "- CIFAR10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class.\n",
    "- There are 50,000 training images and 10,000 test images.\n",
    "- The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
    "- More details can be found at: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"./data\"\n",
    "\n",
    "class_labels = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "denorm_fn = torchvision.transforms.Normalize((-1, -1, -1), (2, 2, 2))\n",
    "\n",
    "class DataModuleCIFAR10(lightning.LightningDataModule):\n",
    "    \"\"\"PyTorch Lightning data module\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int):\n",
    "        \"\"\"Data module for the simple linear regression dataset\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.train_dataset = torchvision.datasets.CIFAR10(\n",
    "            root=DATAPATH, train=True, download=True, transform=transform\n",
    "        )\n",
    "        self.val_dataset = torchvision.datasets.CIFAR10(\n",
    "            root=DATAPATH, train=False, download=True, transform=transform\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Training data loader\"\"\"\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Validation data loader\"\"\"\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModuleCIFAR10(batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning: Train a ResNet\n",
    "\n",
    "In this section, we will finetune a ResNet18 model using PyTorch Lightning. We will demonstrate the usage of PyTorch Lightning for training and validation. We will also show how to monitor the training process using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning.pytorch.seed_everything(42, workers=True)\n",
    "\n",
    "hyperparameters = {\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-7,\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"step_size\": 5,\n",
    "        \"gamma\": 0.1,\n",
    "    },\n",
    "}\n",
    "\n",
    "resnet = demo.lightning.models.ResNet18(\n",
    "    num_classes=len(class_labels),\n",
    "    class_labels=class_labels,\n",
    "    denorm_fn=denorm_fn,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "trainer = lightning.pytorch.Trainer(\n",
    "    max_epochs=10,\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model=resnet, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning: Hyperparameter Search\n",
    "\n",
    "In this section, we will demonstrate how to perform hyperparameter search using PyTorch Lightning. We will randomly generate some sets of hyperparameters and train the model with these hyperparameters. Then we visualize the results using TensorBoard.\n",
    "\n",
    "#### Note:\n",
    "- This section will run for around 5 minutes (reference gpu: RTX 4060 ti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"optimizer\": {\n",
    "        # [low, high]\n",
    "        \"lr\": [1e-7, 1e-5],\n",
    "        \"weight_decay\": [0.01, 0.1],\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        # [low, high]\n",
    "        \"step_size\": [5, 10],\n",
    "        \"gamma\": [0.1, 0.5],\n",
    "    },\n",
    "}\n",
    "\n",
    "n_samples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hyperparameters():\n",
    "    \"\"\"Generate hyperparameters\"\"\"\n",
    "\n",
    "    hyperparameters = {}\n",
    "    for key, value in search_space.items():\n",
    "        hyperparameters[key] = {}\n",
    "        for subkey, subvalue in value.items():\n",
    "            hyperparameters[key][subkey] = np.random.uniform(subvalue[0], subvalue[1])\n",
    "    return hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(n_samples):\n",
    "    hyperparameters = generate_hyperparameters()\n",
    "    resnet = demo.lightning.models.ResNet18(\n",
    "        num_classes=len(class_labels),\n",
    "        class_labels=class_labels,\n",
    "        denorm_fn=denorm_fn,\n",
    "        hyperparameters=hyperparameters,\n",
    "    )\n",
    "    trainer = lightning.pytorch.Trainer(\n",
    "        max_epochs=10,\n",
    "        log_every_n_steps=1,\n",
    "        check_val_every_n_epoch=1,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=resnet, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning: Built-in Callbacks\n",
    "\n",
    "In this section, we will demonstrate the usage of PyTorch Lightning built-in callbacks. We will use the following callbacks:\n",
    "- ModelCheckpoint\n",
    "- EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    lightning.pytorch.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=1,\n",
    "        verbose=True,\n",
    "    ),\n",
    "    lightning.pytorch.callbacks.ModelCheckpoint(\n",
    "        dirpath=\"./models\",\n",
    "        filename=\"resnet18_cifar10_best_{epoch:02d}\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "        verbose=True,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-1,\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"step_size\": 5,\n",
    "        \"gamma\": 0.1,\n",
    "    },\n",
    "}\n",
    "\n",
    "resnet = demo.lightning.models.ResNet18(\n",
    "    num_classes=len(class_labels),\n",
    "    class_labels=class_labels,\n",
    "    denorm_fn=denorm_fn,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "trainer = lightning.pytorch.Trainer(\n",
    "    max_epochs=10,\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "trainer.fit(model=resnet, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning: Customized Callbacks\n",
    "\n",
    "In this section, we will create a customized callback for PyTorch Lightning. With the implemented callback, we will be able to monitor the training process on Slack and terminate the training process remotely. This showcases the flexibility of PyTorch Lightning in customizing the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_client = demo.slack.client.SlackClient()\n",
    "callbacks = [\n",
    "    demo.lightning.callbacks.slack.MonitorTrainingOnSlack(\n",
    "        slack_client=slack_client, log_every_n_epoch=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = generate_hyperparameters()\n",
    "\n",
    "resnet = demo.lightning.models.ResNet18(\n",
    "    num_classes=len(class_labels),\n",
    "    class_labels=class_labels,\n",
    "    denorm_fn=denorm_fn,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "trainer = lightning.pytorch.Trainer(\n",
    "    max_epochs=50,\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "trainer.fit(model=resnet, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
